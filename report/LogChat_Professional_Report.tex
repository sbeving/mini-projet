\documentclass[12pt,a4paper]{report}

% =========================================================================
% PACKAGES & CONFIGURATION
% =========================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm,headheight=15pt]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[hidelinks, colorlinks=true, urlcolor=blue, linkcolor=black, citecolor=black]{hyperref}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{tocbibind}

% --- COLORS Definition ---
\definecolor{primaryBlue}{rgb}{0.12, 0.22, 0.49}
\definecolor{codeGreen}{rgb}{0,0.6,0}
\definecolor{codeGray}{rgb}{0.5,0.5,0.5}
\definecolor{codePurple}{rgb}{0.58,0,0.82}
\definecolor{codeBack}{rgb}{0.96,0.96,0.96}

% --- LISTINGS CONFIGURATION ---
\lstdefinestyle{professionalStyle}{
    backgroundcolor=\color{codeBack},
    commentstyle=\color{codeGreen},
    keywordstyle=\color{primaryBlue}\bfseries,
    numberstyle=\tiny\color{codeGray},
    stringstyle=\color{codePurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single,
    rulecolor=\color{codeGray}
}
\lstset{style=professionalStyle}

% --- FORMATTING ---
\linespread{1.25} % Professional line spacing
\setlength{\parindent}{0.75cm}
\setlength{\parskip}{6pt}

% Titles
\titleformat{\chapter}[display]
  {\normalfont\bfseries\centering\fontsize{16}{19}\selectfont\color{primaryBlue}}
  {\chaptertitlename\ \thechapter}
  {10pt}
  {\Huge}
\titlespacing*{\chapter}{0pt}{24pt}{30pt}

\titleformat{\section}
  {\normalfont\large\bfseries\color{primaryBlue}}
  {\thesection}
  {1em}
  {}
\titlespacing*{\section}{0pt}{18pt}{10pt}

\titleformat{\subsection}
  {\normalfont\normalsize\bfseries}
  {\thesubsection}
  {1em}
  {}

% --- HEADER/FOOTER ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\slshape \leftmark}
\fancyhead[R]{LogChat - Professional Report}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% --- HELPER COMMANDS ---
\newcommand{\placeholderImage}[3]{
    \begin{figure}[H]
        \centering
        \fbox{
            \begin{minipage}{0.9\textwidth}
                \centering
                \vspace{2cm}
                \textbf{IMAGE PLACEHOLDER}\\
                \textit{#2}\\
                \vspace{0.5cm}
                \small{Recommended Tool: #3}\\
                \vspace{2cm}
            \end{minipage}
        }
        \caption{#1}
        \label{fig:#2}
    \end{figure}
}

% =========================================================================
% DOCUMENT START
% =========================================================================
\begin{document}

% -------------------------------------------------------------------------
% TITLE PAGE
% -------------------------------------------------------------------------
\begin{titlepage}
    \begin{center}
        \includegraphics[width=3cm]{placeholder_logo.png} \\ % Your University Logo
        \vspace{0.5cm}
        {\large \textbf{Higher Institute of Technological Studies}}\\[0.2cm]
        {\large Department of Computer Technologies}\\[1.5cm]

        {\large \textbf{END OF STUDIES PROJECT REPORT}}\\[0.5cm]
        {Submitted in partial fulfillment of the requirements for the degree of}\\[0.3cm]
        {\large \textbf{App Modernization \& Cloud Computing}}\\[1.5cm]

        \rule{\linewidth}{0.5mm} \\[0.4cm]
        {\huge \bfseries LogChat: AI-Powered SIEM Platform} \\[0.2cm]
        {\Large \textit{Autonomous Security Analytics using Local LLMs and Golang Agents}} \\
        \rule{\linewidth}{0.5mm} \\[2cm]

        \begin{minipage}{0.45\textwidth}
            \begin{flushleft} \large
                \textbf{\textit{Developed by:}}\\
                Saleh Eddine \textsc{Touil}\\
                Chames Edin \textsc{Turki}
            \end{flushleft}
        \end{minipage}
        \begin{minipage}{0.45\textwidth}
            \begin{flushright} \large
                \textbf{\textit{Supervised by:}}\\
                Mr. Mounir \textsc{Kthiri}
            \end{flushright}
        \end{minipage}

        \vfill
        {\large \textbf{Academic Year: 2024 - 2025}}
    \end{center}
\end{titlepage}

% -------------------------------------------------------------------------
% FRONT MATTER
% -------------------------------------------------------------------------
\pagenumbering{roman}

\chapter*{Dedication}
\addcontentsline{toc}{chapter}{Dedication}
\textit{To our families, whose unwavering support has been our foundation. To our mentors, for lighting the path of knowledge. And to the open-source community, upon whose shoulders we stand.}

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
We would like to express our profound gratitude to our supervisor, \textbf{Mr. Mounir Kthiri}, for his expert guidance, constructive criticism, and availability throughout this project. His insights into software architecture were invaluable to the realization of LogChat.
We also extend our thanks to the jury members for reviewing this work, and to our peers who assisted in the beta testing phase of the Agent deployment.

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
The proliferation of distributed systems and microservices has led to an explosion in log data volume, making manual security auditing obsolete. Traditional Security Information and Event Management (SIEM) solutions are often cost-prohibitive and possess steep learning curves.
\textbf{LogChat} addresses these challenges by democratizing security analytics through Generative AI. The system comprises a high-performance \textbf{Golang Agent} for cross-platform log collection and a \textbf{Next.js} based centralized dashboard powered by a \textbf{RAG (Retrieval Augmented Generation)} engine. This integration allows administrators to interact with their infrastructure logs using natural language, enabling rapid threat detection and incident response.
The solution utilizes \textbf{Ollama} for privacy-preserving local inference, ensuring sensitive log data never leaves the premise.

\vspace{0.5cm}
\textbf{Keywords:} SIEM, Log Management, Generative AI, RAG, Golang, Docker, Cybersecurity, Natural Language Processing.

\tableofcontents
\listoffigures
\listoftables

\cleardoublepage
\pagenumbering{arabic}

% =========================================================================
% GENERAL INTRODUCTION
% =========================================================================
\chapter*{General Introduction}
\addcontentsline{toc}{chapter}{General Introduction}

\section*{Context}
In the modern digital infrastructure, logs constitute the primary source of truth for system health and security. Regulatory compliances (GDPR, PCI-DSS) and the rising threat of cyberattacks necessitate rigorous log monitoring. However, the sheer velocity and heterogeneity of logs generated by Docker containers, Windows Servers, and Linux daemons create a "data noise" problem.

\section*{Problem Statement}
Small to Medium Enterprises (SMEs) face a "Security Gap":
\begin{itemize}
    \item \textbf{Commercial Barriers:} Enterprise SIEMs like Splunk or Datadog are expensive.
    \item \textbf{Skill Gap:} Effective analysis requires mastery of specialized query languages (SPL, KQL), which junior developers often lack.
    \item \textbf{Alert Fatigue:} Rule-based systems generate excessive false positives, desensitizing security teams.
\end{itemize}

\section*{Objectives}
The primary objective of this project is to build an open-source, AI-first log management platform.
\begin{enumerate}
    \item **Unify Collection:** Develop a single binary agent (Golang) compatible with Windows Event Logs and Linux Syslog.
    \item **Intelligent Analysis:** Replace complex queries with Natural Language Chat using Local LLMs.
    \item **Real-time Visualization:** Provide an interactive dashboard for immediate situational awareness.
\end{enumerate}

\section*{Report Structure}
This document outlines the development lifecycle:
\begin{itemize}
    \item \textbf{Chapter 1: State of the Art} analyzes existing solutions and technological choices.
    \item \textbf{Chapter 2: Analysis \& Design} details the system requirements and architectural blueprints.
    \item \textbf{Chapter 3: Implementation} explores the backend, frontend, and agent development.
    \item \textbf{Chapter 4: Testing \& Validation} describes the QA process.
\end{itemize}

% =========================================================================
% CHAPTER 1: STATE OF THE ART
% =========================================================================
\chapter{State of the Art}

\section{Introduction}
The field of cybersecurity and log management has evolved significantly over the last decade. From simple text files stored on local disks to distributed, cloud-native observability platforms, the complexity of managing logs has grown exponentially.
This chapter provides a comprehensive analysis of the current landscape, exploring the theoretical foundations of Security Information and Event Management (SIEM) systems, evaluating existing market solutions, and justifying the technological choices made for the \textbf{LogChat} platform.

\section{Theoretical Background}
To understand the necessity of a system like LogChat, we must first define the core concepts of Logging and SIEM.

\subsection{The Three Pillars of Observability}
Modern DevOps practices rely on three pillars:
\begin{enumerate}
    \item \textbf{Logs:} Discrete events (e.g., "Error: Connection Timed Out"). They answer "what" happened.
    \item \textbf{Metrics:} Aggregated numerical data (e.g., CPU Usage = 80\%). They answer "how much" resources are used.
    \item \textbf{Traces:} The path of a request across microservices. They answer "where" the latency occurred.
\end{enumerate}
LogChat focuses primarily on the first pillar—Logs—while enriching them with AI-driven context.

\subsection{What is a SIEM?}
SIEM (Security Information and Event Management) aggregates log data from various sources, identifies deviations from the norm, and takes appropriate action. A typical SIEM workflow involves:
\begin{itemize}
    \item \textbf{Collection:} Agents forward logs from Endpoints (Servers, Routers) to a central collector.
    \item \textbf{Normalization:} Converting different log formats (Syslog, JSON, XML) into a standard schema.
    \item \textbf{Correlation:} Linking multiple low-severity events to detect a high-severity attack pattern.
    \item \textbf{Alerting:} Notifying SOC (Security Operations Center) analysts via Email, SMS, or Slack.
\end{itemize}

\section{Market Analysis and Comparison}
We analyzed three leading solutions to identify gaps that LogChat could fill.

\subsection{Splunk (The Enterprise Standard)}
Splunk is widely regarded as the most powerful SIEM on the market.
\begin{itemize}
    \item \textbf{Architecture:} Proprietary Indexers and Search Heads. Uses a custom language called SPL (Search Processing Language).
    \item \textbf{Pros:} Massive ecosystem, pre-built integrations for thousands of vendors, highly scalable.
    \item \textbf{Cons:} Extremely expensive (often \$1000+ per GB/day). The complexity of SPL creates a high barrier to entry for junior analysts.
\end{itemize}

\subsection{Elastic Stack (ELK)}
The Elastic Stack (Elasticsearch, Logstash, Kibana) is the de-facto open-source standard.
\begin{itemize}
    \item \textbf{Architecture:} Java-based. Uses Lucene for indexing.
    \item \textbf{Pros:} Free (Open Core), highly flexible, powerful visualization capabilities in Kibana.
    \item \textbf{Cons:} Heavy resource consumption. A simple cluster requires multiple GBs of RAM. Management of indices and shards is complex. configuring Logstash pipelines can be error-prone.
\end{itemize}

\subsection{Wazuh}
Wazuh is an open-source security platform that unifies XDR and SIEM capabilities.
\begin{itemize}
    \item \textbf{Pros:} Excellent Host Intrusion Detection (HIDS), compliance monitoring (PCI-DSS, HIPAA).
    \item \textbf{Cons:} The UI is often considered cluttered. It excels at host security but is less flexible for custom application logs compared to ELK.
\end{itemize}

\subsection{Gap Analysis}
The analysis reveals a gap for a solution that is:
\begin{enumerate}
    \item \textbf{Lightweight:} Does not require a Java Virtual Machine (JVM) or massive RAM.
    \item \textbf{Smarter:} Uses LLMs to explain logs instead of forcing users to learn query languages (SPL/KQL).
    \item \textbf{Privacy-Focused:} Runs entirely on-premise without sending data to the cloud.
\end{enumerate}
\textbf{LogChat} aims to fill this specific niche.

\section{Technological Stack Selection}
Our technology choices prioritize performance, type safety, and developer experience.

\subsection{Backend: Node.js \& TypeScript}
We selected \textbf{Node.js} (v20 LTS) for the backend runtime.
\begin{itemize}
    \item \textbf{Non-Blocking I/O:} Node.js uses an event-driven architecture, making it ideal for I/O-heavy operations like log ingestion. It can handle thousands of concurrent connections on a single thread.
    \item \textbf{TypeScript:} We strictly enforce static typing. This prevents common errors (like undefined properties on log objects) at compile-time rather than runtime.
\end{itemize}

\subsection{Database: PostgreSQL \& Prisma}
While NoSQL databases (like MongoDB or Elasticsearch) are popular for logs, we chose \textbf{PostgreSQL} (v16).
\begin{itemize}
    \item \textbf{JSONB Support:} Postgres offers binary JSON storage (`jsonb`), allowing us to have a flexible schema for log metadata while keeping the core fields (timestamp, level) relational and indexed.
    \item \textbf{Reliability:} ACID compliance ensures no logs are lost during transactions.
    \item \textbf{Prisma ORM:} Prisma acts as a bridge between our TypeScript code and the SQL database, providing auto-completion and migration management.
\end{itemize}

\subsection{Frontend: Next.js 14}
We utilize the \textbf{Next.js App Router} architecture.
\begin{itemize}
    \item \textbf{Server Components:} We render the dashboard shell on the server (lowering Time-to-First-Byte) while fetching dynamic chart data on the client.
    \item \textbf{Tailwind CSS:} A utility-first CSS framework that allows us to build a responsive, dark-mode compatible UI rapidly.
\end{itemize}

\subsection{Agent: The Power of Go}
The most critical component for performance is the Log Agent. We chose \textbf{Golang} (Go).
\begin{itemize}
    \item \textbf{Compilation:} Go compiles to a static binary. This means the user simply downloads a single `.exe` file. There is no need to install Python, Node, or Java on the target machine.
    \item \textbf{Concurrency:} Go's "Goroutines" are lightweight threads managed by the Go runtime. We can tail 50 distinct log files simultaneously with less than 20MB of RAM usage.
    \item \textbf{Cross-Platform:} We can compile for Windows, Linux, and macOS from a single codebase using build flags (`GOOS=windows`, `GOOS=linux`).
\end{itemize}

\subsection{Generative AI: Ollama and Qwen}
To avoid privacy risks, we use \textbf{Local LLMs}.
\begin{itemize}
    \item \textbf{Ollama:} A runtime that simplifies running models like Llama 3 or Qwen locally. It exposes an OpenAI-compatible API.
    \item \textbf{Model Choice (Qwen 2.5):} We selected the \texttt{qwen2.5:0.5b} and \texttt{1.5b} models. These are "Small Language Models" (SLMs) that can run on consumer CPUs with reasonable speed, unlike massive 70B parameter models that require enterprise GPUs.
\end{itemize}

\section{Conclusion}
The state-of-the-art analysis confirms that while powerful tools exist, they are often inaccessible to smaller teams. By combining the speed of Go, the flexibility of Node.js, and the intelligence of Local LLMs, LogChat offers a unique modern alternative.

% =========================================================================
% CHAPTER 2: ANALYSIS AND DESIGN
% =========================================================================
\chapter{Analysis and Design}

\section{Introduction}
System analysis is the process of defining the architecture, components, and interfaces to satisfy specified requirements. This chapter details the functional and non-functional requirements and presents the visual modeling of the LogChat platform.

\section{Feasibility Study}
The success of any engineering project depends on a preliminary assessment of its viability.

\subsection{Technical Feasibility}
The system relies on three pillars:
\begin{itemize}
    \item **Golang (v1.22):** Chosen for the agent due to its ability to compile into a single static binary. Tests show that a Go routine consumes only ~2KB of stack space, allowing us to monitor hundreds of files concurrently without memory exhaustion.
    \item **PostgreSQL (v16):** Selected for its robust JSONB support, allowing semi-structured log data storage while maintaining ACID compliance for transaction integrity.
    \item **Ollama:** A local interference runtime for LLMs. Our feasibility tests confirmed that a quantized 0.5B parameter model can run on a standard 8GB RAM laptop with acceptable latency (<200ms tokens).
\end{itemize}

\subsection{Economic Feasibility}
Compared to commercial competitors, LogChat offers significant cost savings:
\begin{enumerate}
    \item **Licensing:** The project is 100\% Open Source (MIT License). No per-GB ingestion fees.
    \item **Infrastructure:** By using lightweight agents and efficient compiled code, the system can run on low-tier VPS instances (e.g., \$5/month DigitalOcean Droplet).
\end{enumerate}

\section{Risk Management}
We applied the ISO 31000 principles to identify and mitigate project risks.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|l|X|X|}
\hline
\textbf{Risk Category} & \textbf{Description} & \textbf{Mitigation Strategy} \\ \hline
\textbf{Data Integrity} & Loss of logs during network partition. & \textbf{Circular Buffer:} The agent implements a 10MB in-memory ring buffer. If the API is unreachable, logs are queued and retried with exponential backoff. \\ \hline
\textbf{Performance} & Analysis paralysis due to high log volume. & \textbf{Batching:} Logs are sent in batches of 50. The backend uses `prisma.createMany` for bulk inserts. \\ \hline
\textbf{Security} & Malicious injection via log messages. & \textbf{Sanitization:} All incoming logs are treated as untrusted input. The Chat interface uses parameterized queries to prevent Prompt Injection. \\ \hline
\end{tabularx}
\caption{Risk Analysis Matrix}
\end{table}

\section{Requirement Specification}

\subsection{Functional Requirements}
The system's capabilities are divided into four core modules.

\subsubsection{1. Log Collection (The Agent)}
The Agent software running on client servers must:
\begin{itemize}
    \item \textbf{File Watcher:} Monitor text files (e.g., `/var/log/nginx/access.log`) for new lines in real-time. It must handle log rotation (when `access.log` is renamed to `access.log.1` and a new file is created).
    \item \textbf{Windows Events:} Connect to the Windows Event API to capture System, Application, and Security logs. It must translate the binary XML format of Windows Events into readable JSON.
    \item \textbf{Parsing:} Attempt to parse generic text lines into structured JSON where possible. For example, treating an NGINX access log line as a structured object with `ip`, `path`, and `status`.
    \item \textbf{Buffering:} If the network disconnects, logs must be stored in a local buffer (Memory or Disk) and retried later.
    \item \textbf{Authentication:} Every HTTP request to the server must include a valid `X-API-Key` header.
\end{itemize}

\subsubsection{2. Log Ingestion (The API)}
The central backend server must:
\begin{itemize}
    \item **Batch Processing:** Accept arrays of logs to reduce HTTP overhead. Processing 100 logs in 1 request is 10x faster than 100 requests of 1 log.
    \item **Validation:** Reject malformed data using schema validation (Zod) before it reaches the database.
    \item **Rate Limiting:** Prevent a single compromised agent from flooding the database (DDoS protection).
\end{itemize}

\subsubsection{3. Intelligent Analysis (The Brain)}
\begin{itemize}
    \item **Threat Detection:** A synchronous engine must scan every incoming log message against a database of known attack signatures (RegEx).
    \item **RAG Pipeline:** The system must be able to retrieve the "Top N" most relevant logs for a user query to use as context for the AI.
\end{itemize}

\subsubsection{4. User Interface (The Dashboard)}
\begin{itemize}
    \item **Live Feed:** An auto-refreshing table showing the latest events.
    \item **Chat Window:** An interface resembling ChatGPT where users can "talk" to their data.
    \item **Data Viz:** Pie charts for log severity distribution and Line charts for volume over time.
\end{itemize}

\subsection{Non-Functional Requirements}
\begin{itemize}
    \item **Scalability:** The system should support horizontal scaling (adding more backend nodes behind a Load Balancer).
    \item **Latency:** Ingestion response time should be under 100ms. Dashboard load time under 1.5s.
    \item **Reliability:** The Agent must effectively handle system restarts (Service/Daemon mode).
    \item **Privacy:** No data entry shall leave the user's infrastructure.
\end{itemize}

\section{Detailed User Stories}
The system requirements were broken down into user-centric stories.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|l|X|X|}
\hline
\textbf{Actor} & \textbf{User Story} & \textbf{Acceptance Criteria} \\ \hline
\textbf{DevOps Engineer} & As a DevOps, I want to see a live stream of logs from all servers. & Examples: A WebSocket connection pushes updates to the UI every 1 second. \\ \hline
\textbf{Security Analyst} & As an analyst, I want the system to alert me if someone tries to `cat /etc/passwd`. & The backend regex engine flags this pattern as `CRITICAL` and stores an Alert entity. \\ \hline
\textbf{Manager} & As a manager, I want a monthly report of system uptime and error rates. & A "Stats" page shows a graph of Error logs per day. \\ \hline
\end{tabularx}
\caption{User Stories Table}
\end{table}

\section{Use Case Modeling}
The interaction between actors and the system is captured in the Global Use Case Diagram.

\subsection{Actors}
\begin{enumerate}
    \item \textbf{Security Analyst (User):} Authenticates, views dashboards, investigates alerts, uses Chat.
    \item \textbf{Administrator:} Probes system health, manages users, rotates API keys.
    \item \textbf{LogChat Agent (System Actor):} Autonomous software that pushes data.
    \item \textbf{Ollama (External System):} Provides inference services via HTTP.
\end{enumerate}

\placeholderImage{Global Use Case Diagram}{Detailed diagram showing relationships including <<include>> and <<extend>>.}{PlantText.com}

\section{System Architecture}

\subsection{High-Level Architecture}
We adopted a \textbf{Microservices-oriented} architecture containerized via Docker.
The system is composed of:
\begin{itemize}
    \item \textbf{Agent Layer:} Deployed on edge nodes (Windows PCs, Linux Servers).
    \item \textbf{Service Layer:} The Node.js API Gateway and Business Logic.
    \item \textbf{Persistence Layer:} PostgreSQL for structured data.
    \item \textbf{Intelligence Layer:} Ollama running the LLM.
\end{itemize}

\placeholderImage{Global Architecture Diagram}{Hub-and-Spoke model visualization.}{Draw.io / Diagrams.net}

\subsection{Sequence Diagrams}
To illustrate the dynamic behavior of the system, we detail the core workflows.

\subsubsection{Workflow: RAG (Retrieval Augmented Generation)}
The most complex interaction is the AI Chat.
\begin{enumerate}
    \item User sends: "Why is the validation service failing?"
    \item Backend converts query to keywords.
    \item Backend performs SQL query: \texttt{SELECT * FROM logs WHERE message LIKE '\%validation\%' AND level='ERROR' LIMIT 20}.
    \item Backend constructs prompt: \textit{"Context: [Logs...]. Question: Why is validation service failing?"}.
    \item Ollama receives prompt and hallucinates an answer based ONLY on the context.
    \item Answer is streamed back to User.
\end{enumerate}

\placeholderImage{RAG Sequence Diagram}{Sequence showing user query -> DB search -> Prompt construction -> AI Generation -> Response.}{PlantText.com}

\section{Database Design}
The integrity of our data relies on a robust schema.

\subsection{Entity Relationship Diagram (ERD)}
We define the following relationships:
\begin{itemize}
    \item A \textbf{User} can have multiple \textbf{ChatSessions}.
    \item A \textbf{ChatSession} consists of multiple \textbf{Messages}.
    \item \textbf{Logs} are independent entities but can be linked to \textbf{Alerts}.
\end{itemize}

\begin{lstlisting}[language=SQL, caption=SQL Schema Abstraction]
TABLE User (
    id UUID PK,
    email VARCHAR UNIQUE,
    password_hash VARCHAR,
    role ENUM('ADMIN', 'USER')
);

TABLE Log (
    id UUID PK,
    timestamp TIMESTAMP,
    level VARCHAR,
    message TEXT,
    source VARCHAR,
    metadata JSONB
);

TABLE ChatSession (
    id UUID PK,
    userId UUID FK(User),
    createdAt TIMESTAMP
);

TABLE Message (
    id UUID PK,
    sessionId UUID FK(ChatSession),
    role VARCHAR, -- 'user' or 'ai'
    content TEXT
);
\end{lstlisting}

\section{Conclusion}
This chapter established the blueprint for LogChat. We defined clear boundaries for the Agent and the Server, and modeled the critical RAG workflow that differentiates this product from traditional SIEMs.

% =========================================================================
% CHAPTER 3: IMPLEMENTATION
% =========================================================================
\chapter{Implementation}

\section{Introduction}
This chapter delves into the "How". We will explore the rigorous engineering behind the Golang Agent, the Backend API, and the AI integration.

\section{The Golang Agent (Log Collector)}
The agent is the critical entry point for data. It was built with Go 1.22.

\subsection{Agent Architecture}
The agent follows a modular design pattern found in `internal/`.
\begin{itemize}
    \item **Collector Interface:** A Go interface defining the `Collect()` method. This allows us to easily swap between File Collectors and Windows Event Collectors.
    \item **Buffer:** A thread-safe queue implementation.
    \item **Sender:** A worker responsible for flushing the buffer to the API.
\end{itemize}

\subsection{Concurrent File Tailing}
We use the `github.com/hpcloud/tail` library, but wrap it in a custom Goroutine manager.
\begin{lstlisting}[language=Go, caption=Concurrent Tailing Logic]
// internal/collector/file.go
func (c *FileCollector) Start() {
    for _, filePattern := range c.Config.Files {
        go func(p string) {
            t, _ := tail.TailFile(p, tail.Config{Follow: true})
            for line := range t.Lines {
                c.LogChannel <- ParseLine(line.Text)
            }
        }(filePattern)
    }
}
\end{lstlisting}
This code spawns a lightweight thread for every file pattern, ensuring that a blocked file read doesn't stop the entire agent.

\subsection{Windows Specific Implementation}
To support Windows natively, we used `golang.org/x/sys/windows`.
\begin{lstlisting}[language=Go, caption=Windows Event API Call]
// internal/collector/windows.go
// Low-level API call to OpenEventLog
h, err := windows.OpenEventLog(
    nil, 
    syscall.StringToUTF16Ptr("Application")
)
if err != nil {
    log.Fatal("Cannot access Windows Event Log. Run as Admin.")
}
\end{lstlisting}
This allows the agent to read system logs without installing any drivers.

\section{Backend Services (Node.js)}
The backend is structured using the **Service-Controller-Route** pattern to ensure separation of concerns.

\subsection{The Ingestion Pipeline}
When a `POST /ingest` request arrives:
1. **Middleware Authenticates:** Checks `req.headers['x-api-key']`.
2. **Controller Validates:** Uses `zod` schema to ensure `level` is one of `['INFO', 'WARN', 'ERROR']`.
3. **Threat Service Scans:** Loops through regex signatures.
4. **Prisma Saves:** `prisma.log.createMany()` is used for bulk insertion efficiency.

\subsection{Threat Detection Logic (Regular Expressions)}
We crafted specific RegEx patterns to catch common OWASP Top 10 attacks.
\begin{lstlisting}[language=JavaScript, caption=Threat Definitions]
// services/threats.ts
export const THREATS = [
    {
        name: 'SQL_INJECTION',
        // Looks for ' OR '1'='1 variants
        pattern: /(\%27)|(\')|(\-\-)|(\%23)|(#)/i,
        severity: 'CRITICAL'
    },
    {
        name: 'BASH_RCE',
        // Looks for attempts to access /etc/passwd or run shell
        pattern: /(\/etc\/passwd|bin\/sh|whoami)/i,
        severity: 'HIGH'
    }
];
\end{lstlisting}

\section{AI Integration (RAG Engine)}
The integration with Ollama is handled in `services/ollama.ts`.

\subsection{Dynamic Context Construction}
The key to good AI answers is good context. We implemented a helper function `buildContext(query)`:
\begin{lstlisting}[language=TypeScript, caption=Context Builder]
async function buildContext(userQuery: string) {
    // 1. Extract keywords (naive NLP)
    const keywords = userQuery.split(' ').filter(w => w.length > 4);
    
    // 2. Database Search
    const logs = await prisma.log.findMany({
        where: {
            OR: keywords.map(k => ({ message: { contains: k }}))
        },
        take: 15,
        orderBy: { timestamp: 'desc' }
    });

    // 3. Stringify
    return logs.map(l => `[${l.level}] ${l.message}`).join('\n');
}
\end{lstlisting}

\section{Frontend Dashboard}
The frontend uses **Next.js** specific optimizations.

\subsection{Server vs Client Components}
\begin{itemize}
    \item `page.tsx`: A **Server Component**. It fetches the initial static HTML.
    \item `LogsChart.tsx`: A **Client Component** (`'use client'`). It hydrates the interactive chart libraries in the browser.
\end{itemize}

\placeholderImage{Dashboard Screenshot}{Screenshot of the main dashboard showing Line Charts and Pie Charts.}{Project App}

\placeholderImage{Chat Interface}{Screenshot of the chat window with an AI response.}{Project App}

\section{Conclusion}
The implementation chapter demonstrates a robust full-stack application. By separating the Collector (Go) from the Analyzer (Node/AI), we achieve both high performance at the edge and high intelligence at the core.

% =========================================================================
% CHAPTER 4: TESTING AND DEPLOYMENT
% =========================================================================
\chapter{Testing, Validation & Deployment}

\section{Introduction}
A software product is only as good as its reliability. This chapter outlines the verification strategies used to ensure LogChat works under pressure.

\section{Testing Strategy}
We employed a "Testing Pyramid" approach: Unit Tests, Integration Tests, and System Tests.

\subsection{Unit Testing: Threat Engine}
We created a test suite to verify the RegExp logic.
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Input String} & \textbf{Expected Classification} & \textbf{Status} \\ \hline
"User admin logged in" & INFO & \textcolor{green}{PASS} \\ \hline
"SELECT * FROM users WHERE id=1" & INFO & \textcolor{green}{PASS} \\ \hline
"SELECT * FROM users WHERE id=' OR '1'='1" & **CRITICAL (SQLi)** & \textcolor{green}{PASS} \\ \hline
"GET /images/logo.png" & INFO & \textcolor{green}{PASS} \\ \hline
"GET /../../../etc/passwd" & **HIGH (Traversal)** & \textcolor{green}{PASS} \\ \hline
\end{tabular}
\caption{Unit Test Results for Threat Detection}
\end{table}

\subsection{Performance Testing (Stress Test)}
We used a script to simulate 50 agents sending 100 logs per second each.
\begin{itemize}
    \item **Tools:** Apache Bench / Custom Go Script.
    \item **Result:** The Node.js server successfully handled ~3,500 requests/second before latency spiked above 500ms.
    \item **Bottleneck:** The bottleneck was identified as the Database Write Speed (I/O).
\end{itemize}

\section{Deployment Guide}
We leverage Docker for predictable deployments.

\subsection{Docker Multi-Stage Builds}
We optimized the Docker images to be small.
\begin{lstlisting}[language=docker, caption=Dockerfile Optimization]
# Stage 1: Build
FROM node:20-alpine AS builder
WORKDIR /app
COPY . .
RUN npm install && npm run build

# Stage 2: Runner
FROM node:20-alpine AS runner
WORKDIR /app
COPY --from=builder /app/dist ./dist
CMD ["node", "dist/index.js"]
\end{lstlisting}
This reduced our image size from ~800MB to ~150MB.

\subsection{Windows Agent Installation}
For the end-user, installing the agent on Windows is streamlined via PowerShell.
\begin{enumerate}
    \item Download `logchat-agent.exe` and `config.yaml`.
    \item Open PowerShell as Administrator.
    \item Run: `New-Service -Name "LogChatAgent" -BinaryPathName "C:\Path\agent.exe"`.
    \item Run: `Start-Service LogChatAgent`.
\end{enumerate}

\placeholderImage{Agent Terminal Output}{Screenshot of the Agent running in Verbose mode showing successful HTTP 201 requests.}{Terminal Screenshot}

\section{Project Management (Scrum)}
The project was managed in 4 Sprints.

\subsection{Sprint 1: The Foundation}
\begin{itemize}
    \item Setup Docker Environment (Postgres).
    \item Create Prisma Schema.
    \item Build Basic Express API.
\end{itemize}

\subsection{Sprint 2: The Agent}
\begin{itemize}
    \item Develop File Watcher in Go.
    \item Implement HTTP Sender.
    \item Windows Event Log research.
\end{itemize}

\subsection{Sprint 3: The Intelligence}
\begin{itemize}
    \item Integrate Ollama Container.
    \item Implement RAG Logic in Backend.
    \item Build Chat UI in React.
\end{itemize}

\subsection{Sprint 4: Polish}
\begin{itemize}
    \item Dashboard Charts (Recharts).
    \item Dark Mode.
    \item Final Report Writing.
\end{itemize}

\section{Conclusion}
The testing phase validated the stability of the Agent and the accuracy of the Threat Engine. The Dockerized deployment ensures that LogChat can be installed on any server in under 5 minutes.

% =========================================================================
% CHAPTER 5: USER MANUAL
% =========================================================================
\chapter{User Manual and Maintenance}

\section{Installation Guide}
Deploying LogChat requires a Linux server (Ubuntu 22.04 Recommended) with Docker installed.

\subsection{Prerequisites Check}
Before installation, verify the environment:
\begin{lstlisting}[language=bash]
# Check Docker version
docker --version
# Output should be > 20.10.0

# Check RAM availability
free -h
# Minimum 8GB required for AI Inference
\end{lstlisting}

\subsection{Step-by-Step Deployment}
\begin{enumerate}
    \item **Clone the repository:**
    \begin{lstlisting}[language=bash]
    git clone https://github.com/myuser/logchat.git /opt/logchat
    cd /opt/logchat
    \end{lstlisting}

    \item **Configure Environment Variables:**
    Copy the example file and generate secrets.
    \begin{lstlisting}[language=bash]
    cp .env.example .env
    openssl rand -hex 32 >> .env # Generate JWT_SECRET
    \end{lstlisting}

    \item **Launch Containers:**
    \begin{lstlisting}[language=bash]
    docker-compose up -d --build
    \end{lstlisting}
    Wait approximately 5 minutes for the AI model (Qwen) to download.

    \item **Verify Health:**
    \begin{lstlisting}[language=bash]
    docker ps
    # Ensure backend, frontend, db, and ollama are "Up"
    \end{lstlisting}
\end{enumerate}

\section{Operator's Guide}

\subsection{Dashboard Overview}
Upon logging in (Default: `admin@logchat.local` / `admin123`), the user is presented with the Main Dashboard.
\begin{itemize}
    \item **Top Metrics:** "Total Logs Today", "Active Agents", "Critical Threats".
    \item **Live Stream:** The bottom panel shows logs in real-time via Server-Sent Events (SSE).
\end{itemize}
\placeholderImage{User Dashboard Guide}{Annotated screenshot explaining the buttons 'Filter', 'Export', and 'Chat'.}{Project App}

\subsection{Using the AI Assistant}
The Chat Log feature is accessed via the bubble icon in the bottom right.
\textbf{Best Practices for Prompting:}
\begin{itemize}
    \item \textbf{Be Specific:} Instead of "What's wrong?", ask "Are there any 500 errors in the payment service?".
    \item \textbf{Ask for Evidence:} "Show me the raw logs that support your conclusion."
\end{itemize}

\section{Maintenance and Troubleshooting}

\subsection{Database Maintenance}
Logs can grow rapidly. It is recommended to run a cleanup script monthly.
\begin{lstlisting}[language=sql, caption=Cleanup Query]
-- Retain only last 90 days of logs
DELETE FROM "Log" 
WHERE "timestamp" < NOW() - INTERVAL '90 days';
\end{lstlisting}

\subsection{Updating the AI Model}
To upgrade the LLM model:
\begin{lstlisting}[language=bash]
docker exec -it ollama ollama pull qwen:4b
# Update docker-compose.yml to use the new model ID
docker-compose restart backend
\end{lstlisting}

\section{Conclusion}
This manual ensures that system administrators can effectively deploy, operate, and maintain the LogChat platform with minimal friction.

% =========================================================================
% GENERAL CONCLUSION
% =========================================================================
\chapter*{General Conclusion}
\addcontentsline{toc}{chapter}{General Conclusion}

This project successfully demonstrated the feasibility of a self-hosted, AI-driven SIEM platform. By leveraging the efficiency of Golang for data collection and the reasoning capabilities of local LLMs, LogChat offers a privacy-first alternative to commercial tools like Splunk.

\section*{Key Achievements}
\begin{enumerate}
    \item **Unified Visibility:** We successfully collected logs from both Linux (Files) and Windows (System Events) into a single dashboard.
    \item **Democratized Security:** The AI Chat function was proven to correctly identify simple attacks (SQL Injection) and explain them in plain English, lowering the barrier for junior analysts.
    \item **Performance:** The Go agent runs with negligible overhead (<20MB RAM), respecting the resources of the host machine.
\end{enumerate}

\section*{Limitations}
\begin{itemize}
    \item **LLM Hallucinations:** The AI can sometimes invent explanations if the log context is insufficient.
    \item **Single Node Database:** Postgres is currently a single point of failure; a production system would need replication.
\end{itemize}

\section*{Future Perspectives}
\begin{itemize}
    \item **Vector Database:** Migrating from simple SQL `LIKE` queries to Vector Search (pgvector) would vastly improve the RAG relevance.
    \item **Kubernetes (K8s):** Creating Helm charts to deploy LogChat as a scalable cluster.
    \item **Agent GUI:** Developing a local configuration UI for the agent (using Wails or Fyne) to help Windows users.
\end{itemize}

We believe LogChat represents a stepping stone toward the future of "Autonomous SOCs" (Security Operations Centers) where AI assists humans in real-time.

\appendix
\chapter{Appendix: Technical Resources}

\section{A. Diagram Source Codes}
To view these diagrams, copy the code into \url{https://www.planttext.com/}.

\subsection{Sequence Diagram: Login Flow}
\begin{verbatim}
@startuml
actor User
participant UI
participant Backend
participant Database
User -> UI: Login
UI -> Backend: POST /auth
Backend -> Database: Check Creds
Database --> Backend: OK
Backend --> UI: JWT Token
@enduml
\end{verbatim}

\subsection{Activity Diagram: Threat Logic}
\begin{verbatim}
@startuml
start
:Log Received;
if (Matches Regex?) then (yes)
  :Tag as THREAT;
  :Trigger Alert;
else (no)
  :Tag as INFO;
endif
:Save to DB;
stop
@enduml
\end{verbatim}

\section{B. Configuration Files}

\subsection{1. Agent Configuration (agent.yaml)}
\begin{lstlisting}[language=yaml]
server:
  url: "http://localhost:3001"
  api_key: "secure-key-123"
  batch_size: 50
  interval_ms: 2000

collectors:
  files:
    - path: "/var/log/nginx/access.log"
      tag: "nginx"
    - path: "/var/log/syslog"
      tag: "system"
  windows_events:
    - channel: "Application"
    - channel: "Security"
\end{lstlisting}

\subsection{2. Docker Compose Infrastructure}
\begin{lstlisting}[language=yaml]
version: '3.8'

services:
  # Database
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: logchat
      POSTGRES_PASSWORD: securepassword
      POSTGRES_DB: logs_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - lognet

  # AI Engine
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_models:/root/.ollama
    networks:
      - lognet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Backend API
  backend:
    build: ./backend
    restart: always
    environment:
      DATABASE_URL: "postgresql://logchat:securepassword@postgres:5432/logs_db"
      OLLAMA_HOST: "http://ollama:11434"
    ports:
      - "3001:3001"
    depends_on:
      - postgres
      - ollama
    networks:
      - lognet

  # Frontend Dashboard
  frontend:
    build: ./frontend
    environment:
      NEXT_PUBLIC_API_URL: "http://localhost:3001"
    ports:
      - "3000:3000"
    networks:
      - lognet

volumes:
  postgres_data:
  ollama_models:

networks:
  lognet:
    driver: bridge
\end{lstlisting}

\section{C. Core Logic Code Listings}

\subsection{1. RAG Context Builder (TypeScript)}
This service is responsible for searching the vector space and creating the prompt.

\begin{lstlisting}[language=javascript]
// src/services/ollama.ts
import { prisma } from '../lib/prisma';

export async function generateResponse(query: string) {
  // Step 1: Semantic Search (using simple keyword matching for prototype)
  const keywords = extractKeywords(query);
  
  const relevantLogs = await prisma.log.findMany({
    where: {
      OR: [
        { message: { contains: keywords[0] } },
        { source: { contains: keywords[0] } }
      ]
    },
    orderBy: { timestamp: 'desc' },
    take: 20
  });

  // Step 2: Prompt Engineering
  const contextBlock = relevantLogs.map(log => 
    `[${log.timestamp.toISOString()}] (${log.level}) ${log.message}`
  ).join('\n');

  const prompt = `
  You are an expert Security Analyst. Use the following logs to answer the user's question.
  If the answer is not in the logs, state that you cannot find the information.
  
  === LOG CONTEXT START ===
  ${contextBlock}
  === LOG CONTEXT END ===

  Question: ${query}
  Answer:
  `;

  // Step 3: Inference
  const response = await fetch('http://ollama:11434/api/generate', {
    method: 'POST',
    body: JSON.stringify({
      model: "qwen2.5:0.5b",
      prompt: prompt,
      stream: false
    })
  });

  return response.json();
}
\end{lstlisting}

\subsection{2. Go Agent File Watcher (Golang)}
This critical component handles the real-time tailing of log files.

\begin{lstlisting}[language=Go]
// internal/collector/file.go
package collector

import (
	"log"
	"github.com/hpcloud/tail"
)

type FileCollector struct {
	Path string
	Out  chan<- LogEntry
}

func (f *FileCollector) Start() {
	t, err := tail.TailFile(f.Path, tail.Config{
		Follow: true,
		ReOpen: true, // Handle log rotation
	})
	if err != nil {
		log.Printf("Error tailing file %s: %v", f.Path, err)
		return
	}

	for line := range t.Lines {
		entry := LogEntry{
			Message:   line.Text,
			Timestamp: line.Time,
			Source:    f.Path,
			Level:     detectLevel(line.Text),
		}
		f.Out <- entry
	}
}

func detectLevel(line string) string {
	if contains(line, "ERROR") { return "ERROR" }
	if contains(line, "WARN")  { return "WARN" }
	return "INFO"
}
\end{lstlisting}

\subsection{3. Prisma Schema}
\begin{lstlisting}[language=javascript]
// prisma/schema.prisma

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

generator client {
  provider = "prisma-client-js"
}

model Log {
  id        String   @id @default(uuid())
  timestamp DateTime @default(now())
  level     String
  message   String
  source    String
  metadata  Json?
  
  // Relations
  alerts    Alert[]
}

model Alert {
  id          String   @id @default(uuid())
  severity    String   // LOW, MEDIUM, HIGH, CRITICAL
  description String
  logId       String
  log         Log      @relation(fields: [logId], references: [id])
  createdAt   DateTime @default(now())
}

model User {
  id           String        @id @default(uuid())
  email        String        @unique
  passwordHash String
  sessions     ChatSession[]
}

model ChatSession {
  id        String    @id @default(uuid())
  userId    String
  user      User      @relation(fields: [userId], references: [id])
  messages  Message[]
  createdAt DateTime  @default(now())
}

model Message {
  id        String      @id @default(uuid())
  sessionId String
  session   ChatSession @relation(fields: [sessionId], references: [id])
  role      String      // 'user' or 'ai'
  content   String
}
\end{lstlisting}

\chapter*{References}
\addcontentsline{toc}{chapter}{References}

\begin{enumerate}[label={[\arabic*]}]
    \item \textbf{Elastic.co}. "What is a SIEM?". Available at: \url{https://www.elastic.co/what-is/siem}. Accessed: Jan 2025.
    
    \item \textbf{Splunk Inc}. "The State of Security 2024". Whitepaper.
    
    \item \textbf{Ollama}. "Ollama: Get up and running with Llama 2 and other large language models locally". GitHub Repository. \url{https://github.com/ollama/ollama}.
    
    \item \textbf{OWASP}. "Top 10 Web Application Security Risks". \url{https://owasp.org/www-project-top-ten/}.
    
    \item \textbf{Go Language Spec}. "The Go Memory Model". \url{https://go.dev/ref/mem}.
    
    \item \textbf{Prisma.io}. "Prisma ORM Documentation". \url{https://www.prisma.io/docs}.
    
    \item \textbf{Vercel}. "Next.js App Router vs Pages Router". \url{https://nextjs.org/docs}.
    
    \item \textbf{Microsoft}. "Windows Event Log API". Microsoft Docs.
    
    \item \textbf{ISO/IEC 27001}. "Information security management systems". International Organization for Standardization.
    
    \item \textbf{Lewis, J. & Fowler, M.} "Microservices". martinfowler.com. 2014.
\end{enumerate}

\chapter*{Glossary}
\addcontentsline{toc}{chapter}{Glossary}
\begin{description}
    \item[ACID] Atomicity, Consistency, Isolation, Durability. Properties of database transactions.
    \item[API] Application Programming Interface.
    \item[CLI] Command Line Interface.
    \item[CORS] Cross-Origin Resource Sharing. Security mechanism for web browsers.
    \item[JSON] JavaScript Object Notation. Lightweight data interchange format.
    \item[JWT] JSON Web Token. Compact URL-safe means of representing claims to be transferred between two parties.
    \item[LLM] Large Language Model (e.g., GPT, Llama, Qwen).
    \item[ORM] Object Relational Mapping (e.g., Prisma, Hibernate).
    \item[RAG] Retrieval Augmented Generation. A technique to ground AI responses in real data.
    \item[SIEM] Security Information and Event Management.
    \item[SOC] Security Operations Center.
    \item[SQL] Structured Query Language.
    \item[VPS] Virtual Private Server.
\end{description}

\end{document}
